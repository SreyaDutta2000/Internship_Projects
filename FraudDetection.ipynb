{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sBJBfSYfpH7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"Load and preprocess the fraud detection dataset.\"\"\"\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Convert datetime column to proper format\n",
        "    df['TX_DATETIME'] = pd.to_datetime(df['TX_DATETIME'])\n",
        "\n",
        "    # Create features based on fraud patterns\n",
        "    # Feature 1: High amount transactions (> 220)\n",
        "    df['HIGH_AMOUNT'] = (df['TX_AMOUNT'] > 220).astype(int)\n",
        "\n",
        "    # Feature 2: Terminal fraud patterns\n",
        "    # Group by terminal and date to track suspicious patterns\n",
        "    terminal_stats = df.groupby(['TERMINAL_ID', pd.Grouper(key='TX_DATETIME', freq='D')])['TX_FRAUD']\\\n",
        "        .mean().reset_index()\n",
        "    terminal_stats.columns = ['TERMINAL_ID', 'TX_DATETIME', 'TERMINAL_FRAUD_RATE']\n",
        "    df = pd.merge(df, terminal_stats, on=['TERMINAL_ID', 'TX_DATETIME'], how='left')\n",
        "\n",
        "    # Feature 3: Customer fraud patterns\n",
        "    # Track customer transaction amounts and detect anomalies\n",
        "    df['CUSTOMER_TX_COUNT'] = df.groupby(['CUSTOMER_ID', pd.Grouper(key='TX_DATETIME', freq='D')])\\\n",
        "        ['TX_AMOUNT'].transform('count')\n",
        "    df['CUSTOMER_TX_AMOUNT_MEAN'] = df.groupby(['CUSTOMER_ID', pd.Grouper(key='TX_DATETIME', freq='D')])\\\n",
        "        ['TX_AMOUNT'].transform('mean')\n",
        "\n",
        "    # Create time-based features\n",
        "    df['HOUR'] = df['TX_DATETIME'].dt.hour\n",
        "    df['DAY_OF_WEEK'] = df['TX_DATETIME'].dt.dayofweek\n",
        "\n",
        "    # Select features for modeling\n",
        "    features = ['TX_AMOUNT', 'HIGH_AMOUNT', 'TERMINAL_FRAUD_RATE',\n",
        "               'CUSTOMER_TX_COUNT', 'CUSTOMER_TX_AMOUNT_MEAN',\n",
        "               'HOUR', 'DAY_OF_WEEK']\n",
        "\n",
        "    X = df[features]\n",
        "    y = df['TX_FRAUD']\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, df\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    \"\"\"Train the Random Forest Classifier.\"\"\"\n",
        "    # Initialize and train the model\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "    rf.fit(X_train, y_train)\n",
        "    return rf\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Evaluate the model performance.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Calculate AUC-ROC score\n",
        "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
        "    print(f\"\\nAUC-ROC Score: {auc_roc:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "               xticklabels=['Legitimate', 'Fraud'],\n",
        "               yticklabels=['Legitimate', 'Fraud'])\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot feature importances\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    importances = pd.Series(model.feature_importances_, index=X_test.columns)\n",
        "    importances.sort_values(ascending=False).plot(kind='bar')\n",
        "    plt.title('Feature Importances')\n",
        "    plt.ylabel('Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess the data\n",
        "    file_path = 'fraud_transactions.csv'  # You'll need to provide your dataset path\n",
        "    X_train, X_test, y_train, y_test, df = load_and_preprocess_data(file_path)\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Training model...\")\n",
        "    model = train_model(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"\\nEvaluating model...\")\n",
        "    evaluate_model(model, X_test, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}